{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUD4ylfaLZA6"
   },
   "source": [
    "# Model: Cobra v0\n",
    "\n",
    "**Model Parameters:**\n",
    "* LightGBM\n",
    "* Trained on large features\n",
    "* Top 60 riskiest features neuralized\n",
    "* Params:\n",
    "    * \"n_estimators\": 2000\n",
    "    * \"learning_rate\": 0.01\n",
    "    * \"max_depth\": 5\n",
    "    * \"num_leaves\": 2 ** 5\n",
    "    * \"colsample_bytree\": 0.1\n",
    "\n",
    "**Note:**\n",
    "* This is the base model with the numerai given params\n",
    "* Updated: 2022-04-29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dioJyoTnxEm3"
   },
   "source": [
    "---\n",
    "# Load data & libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP VERSION\n",
    "MODEL_NAME = \"cobra\"\n",
    "N_FEATURES_NEUTRALIZED = 60\n",
    "VERSION = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "MaNpcfNkKUg_"
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "from numerapi import NumerAPI\n",
    "from utils.utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    get_biggest_change_features,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    ")\n",
    "from utils.api_keys import (\n",
    "    PUBLIC_ID,\n",
    "    SECRET_KEY,\n",
    "    COBRA_MODEL_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current round #: 313\n"
     ]
    }
   ],
   "source": [
    "# instantiate numerai api\n",
    "napi = NumerAPI(public_id=PUBLIC_ID, secret_key=SECRET_KEY)\n",
    "current_round = napi.get_current_round()\n",
    "print(f\"Current round #: {current_round}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data\n"
     ]
    }
   ],
   "source": [
    "# read the feature metadata and get the \"large\" feature set\n",
    "print('Reading training data...')\n",
    "\n",
    "with open(\"data/features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "small_features = feature_metadata[\"feature_sets\"][\"small\"]\n",
    "medium_features = feature_metadata[\"feature_sets\"][\"medium\"]\n",
    "all_features_values = feature_metadata[\"feature_sets\"].values()\n",
    "all_features = list(set(list(itertools.chain(*all_features_values))))\n",
    "features = [x for x in all_features if x not in small_features and x not in medium_features]\n",
    "read_columns = features + [ERA_COL, DATA_TYPE_COL, TARGET_COL]\n",
    "\n",
    "training_data = pd.read_parquet('data/train.parquet', columns=read_columns)\n",
    "validation_data = pd.read_parquet('data/validation.parquet', columns=read_columns)\n",
    "live_data = pd.read_parquet(f'data/live_{current_round}.parquet', columns=read_columns)\n",
    "\n",
    "print('...Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the per era correlation of each feature vs the target\n",
    "all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
    "    lambda era: era[features].corrwith(era[TARGET_COL])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get riskiest features (biggest change in correlation to target between h1 era vs h2 era)\n",
    "riskiest_features = get_biggest_change_features(all_feature_corrs, N_FEATURES_NEUTRALIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"garbage collection\" (gc) gets rid of unused data and frees up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dh_cobra_v0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model name\n",
    "model_name = f\"dh_{MODEL_NAME}_v{VERSION}\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model: dh_cobra_v0...\n",
      "Model not found, creating new one\n",
      "Training model: dh_cobra_v0...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3040\n",
      "[LightGBM] [Info] Number of data points in the train set: 2420521, number of used features: 608\n",
      "[LightGBM] [Info] Start training from score 0.500001\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Saving new model: dh_cobra_v0...\n"
     ]
    }
   ],
   "source": [
    "# check existing model, if not, train it\n",
    "print(f\"Checking for existing model: {model_name}...\")\n",
    "model = load_model(model_name)\n",
    "\n",
    "if not model:\n",
    "    print(f\"Model not found, creating new one\")\n",
    "    params = {\"n_estimators\": 2000,\n",
    "              \"learning_rate\": 0.01,\n",
    "              \"max_depth\": 5,\n",
    "              \"num_leaves\": 2 ** 5,\n",
    "              \"colsample_bytree\": 0.1,\n",
    "              \"verbosity\": 1,\n",
    "              \"n_jobs\": -1}\n",
    "    model = LGBMRegressor(**params)\n",
    "    \n",
    "    print(f\"Training model: {model_name}...\")\n",
    "    model.fit(\n",
    "        training_data.filter(like='feature_', axis='columns'),\n",
    "        training_data[TARGET_COL]\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving new model: {model_name}...\")\n",
    "    save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear memory of unused data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nans per column this week: target_nomi_v4_20    5324\n",
      "dtype: int64\n",
      "out of 5324 total rows\n",
      "filling nans with 0.5\n"
     ]
    }
   ],
   "source": [
    "# check for nans\n",
    "nans_per_col = live_data[live_data[\"data_type\"] == \"live\"].isna().sum()\n",
    "\n",
    "if nans_per_col.any():\n",
    "    total_rows = len(live_data[live_data[\"data_type\"] == \"live\"])\n",
    "    print(f\"Number of nans per column this week: {nans_per_col[nans_per_col > 0]}\")\n",
    "    print(f\"out of {total_rows} total rows\")\n",
    "    print(f\"filling nans with 0.5\")\n",
    "    live_data.loc[:, features] = live_data.loc[:, features].fillna(0.5)\n",
    "else:\n",
    "    print(\"No nans in the features this week!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature name\n",
    "model_expected_features = model.booster_.feature_name()\n",
    "\n",
    "if set(model_expected_features) != set(features):\n",
    "    print(f\"New features are available! Might want to retrain model {model_name}.\")\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
    "    validation_data.loc[:, model_expected_features])\n",
    "live_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
    "    live_data.loc[:, model_expected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear memory of unused data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutralize our predictions to the riskiest features\n",
    "validation_data[f\"preds_{model_name}_neutral_riskiest_{N_FEATURES_NEUTRALIZED}\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"preds_{model_name}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "\n",
    "live_data[f\"preds_{model_name}_neutral_riskiest_{N_FEATURES_NEUTRALIZED}\"] = neutralize(\n",
    "    df=live_data,\n",
    "    columns=[f\"preds_{model_name}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_{N_FEATURES_NEUTRALIZED}\"\n",
    "\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "live_data[\"prediction\"] = live_data[model_to_submit].rank(pct=True)\n",
    "validation_data[\"prediction\"].to_csv(f\"predictions/{model_name}_val_preds_{current_round}.csv\")\n",
    "live_data[\"prediction\"].to_csv(f\"predictions/{model_name}_live_preds_{current_round}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Submit predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 14:54:42,254 INFO numerapi.base_api: uploading diagnostics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted validation prediction file 'dh_cobra_v0_val_preds_313.csv'\n"
     ]
    }
   ],
   "source": [
    "# submit validation file for diagnostics\n",
    "napi.upload_diagnostics(\n",
    "    file_path=f\"predictions/{model_name}_val_preds_{current_round}.csv\",\n",
    "    model_id=COBRA_MODEL_ID\n",
    ")\n",
    "\n",
    "print(f\"Submitted validation prediction file '{model_name}_val_preds_{current_round}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit live predictions\n",
    "napi.upload_predictions(\n",
    "    file_path=f\"predictions/{model_name}_live_preds_{current_round}.csv\", \n",
    "    model_id=COBRA_MODEL_ID\n",
    ")\n",
    "\n",
    "print(f\"Submitted live prediction file '{model_name}_live_preds_{current_round}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "numerai_base_model_v1",
   "provenance": []
  },
  "interpreter": {
   "hash": "0126e27fe55ec969de126c2cfdac2c99da69ebfdfbbeb83738b0925389a2d696"
  },
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
