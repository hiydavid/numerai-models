{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUD4ylfaLZA6"
   },
   "source": [
    "# Base Model\n",
    "\n",
    "Updated: 2022-04-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dioJyoTnxEm3"
   },
   "source": [
    "---\n",
    "# Load data & libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MaNpcfNkKUg_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "from numerapi import NumerAPI\n",
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    get_biggest_change_features,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current round #: 311\n",
      "\r"
     ]
    }
   ],
   "source": [
    "# instantiate numerai api\n",
    "napi = NumerAPI()\n",
    "current_round = napi.get_current_round()\n",
    "print(f\"Current round #: {current_round}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal training data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/__/dzz62y2949d5pjv8r63g_3n40000gn/T/ipykernel_37848/2075968534.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/validation.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlive_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/live_{current_round}.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mparquet_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparquet_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparquet_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/fastparquet/api.py\u001b[0m in \u001b[0;36mto_pandas\u001b[0;34m(self, columns, categories, filters, index, row_filter)\u001b[0m\n\u001b[1;32m    724\u001b[0m                             else v[start:start + thislen])\n\u001b[1;32m    725\u001b[0m                      for (name, v) in views.items()}\n\u001b[0;32m--> 726\u001b[0;31m             self.read_row_group_file(rg, columns, categories, index,\n\u001b[0m\u001b[1;32m    727\u001b[0m                                      \u001b[0massign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition_meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                      row_filter=sel, infile=infile)\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/fastparquet/api.py\u001b[0m in \u001b[0;36mread_row_group_file\u001b[0;34m(self, rg, columns, categories, index, assign, partition_meta, row_filter, infile)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         core.read_row_group(\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mselfmade\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselfmade\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/fastparquet/core.py\u001b[0m in \u001b[0;36mread_row_group\u001b[0;34m(file, rg, columns, categories, schema_helper, cats, selfmade, index, assign, scheme, partition_meta, row_filter)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0massign\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Going with pre-allocation!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     read_row_group_arrays(file, rg, columns, categories, schema_helper,\n\u001b[0m\u001b[1;32m    609\u001b[0m                           cats, selfmade, assign=assign, row_filter=row_filter)\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/fastparquet/core.py\u001b[0m in \u001b[0;36mread_row_group_arrays\u001b[0;34m(file, rg, columns, categories, schema_helper, cats, selfmade, assign, row_filter)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         read_col(column, schema_helper, file, use_cat=name+'-catdef' in out,\n\u001b[0m\u001b[1;32m    581\u001b[0m                  \u001b[0mselfmade\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselfmade\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                  \u001b[0mcatdef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-catdef'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/fastparquet/core.py\u001b[0m in \u001b[0;36mread_col\u001b[0;34m(column, schema_helper, infile, use_cat, selfmade, assign, catdef, row_filter)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mskip_nulls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         defi, rep, val = read_data_page(infile, schema_helper, ph, cmd,\n\u001b[0m\u001b[1;32m    492\u001b[0m                                         skip_nulls, selfmade=selfmade)\n\u001b[1;32m    493\u001b[0m         \u001b[0mmax_defi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_definition_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_in_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/fastparquet/core.py\u001b[0m in \u001b[0;36mread_data_page\u001b[0;34m(f, helper, header, metadata, skip_nulls, selfmade)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \"\"\"\n\u001b[1;32m    113\u001b[0m     \u001b[0mdaph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_page_header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mraw_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mio_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumpyIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/fastparquet/core.py\u001b[0m in \u001b[0;36m_read_page\u001b[0;34m(file_obj, page_header, column_metadata)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"Read the data page from the given file-object and convert it to raw,\n\u001b[1;32m     19\u001b[0m     uncompressed bytes (if necessary).\"\"\"\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mraw_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_header\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompressed_page_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     raw_bytes = decompress_data(\n\u001b[1;32m     22\u001b[0m         \u001b[0mraw_bytes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/projenv/lib/python3.10/site-packages/fastparquet/cencoding.pyx\u001b[0m in \u001b[0;36mfastparquet.cencoding.NumpyIO.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# read the feature metadata and get the \"medium\" feature set\n",
    "print('Reading minimal training data')\n",
    "\n",
    "with open(\"data/features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "features = feature_metadata[\"feature_sets\"][\"medium\"]\n",
    "# features = feature_metadata[\"feature_sets\"][\"small\"]\n",
    "read_columns = features + [ERA_COL, DATA_TYPE_COL, TARGET_COL]\n",
    "\n",
    "training_data = pd.read_parquet('data/train.parquet', columns=read_columns)\n",
    "validation_data = pd.read_parquet('data/validation.parquet', columns=read_columns)\n",
    "live_data = pd.read_parquet(f'data/live_{current_round}.parquet', columns=read_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the per era correlation of each feature vs the target\n",
    "all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
    "    lambda era: era[features].corrwith(era[TARGET_COL])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get riskiest features\n",
    "riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n",
    "riskiest_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"garbage collection\" (gc) gets rid of unused data and frees up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"base_model\"\n",
    "print(f\"Checking for existing model: {model_name}...\")\n",
    "model = load_model(model_name)\n",
    "\n",
    "if not model:\n",
    "    print(f\"Model not found, creating new one\")\n",
    "    params = {\"max_iter\": 1000,\n",
    "              \"learning_rate\": 0.1,\n",
    "              \"max_depth\": 5,\n",
    "              \"max_leaf_nodes\": 2 ** 5,\n",
    "              \"verbose\": 1}\n",
    "    model = HistGradientBoostingRegressor(**params)\n",
    "    \n",
    "    print(f\"Training model: {model_name}...\")\n",
    "    model.fit(\n",
    "        training_data.filter(like='feature_', axis='columns'),\n",
    "        training_data[TARGET_COL]\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving new model: {model_name}...\")\n",
    "    save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory of unused data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nans\n",
    "nans_per_col = live_data[live_data[\"data_type\"] == \"live\"].isna().sum()\n",
    "\n",
    "if nans_per_col.any():\n",
    "    total_rows = len(live_data[live_data[\"data_type\"] == \"live\"])\n",
    "    print(f\"Number of nans per column this week: {nans_per_col[nans_per_col > 0]}\")\n",
    "    print(f\"out of {total_rows} total rows\")\n",
    "    print(f\"filling nans with 0.5\")\n",
    "    live_data.loc[:, features] = live_data.loc[:, features].fillna(0.5)\n",
    "else:\n",
    "    print(\"No nans in the features this week!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature name\n",
    "model_expected_features = model.feature_names_in_\n",
    "\n",
    "if set(model_expected_features) != set(features):\n",
    "    print(f\"New features are available! Might want to retrain model {model_name}.\")\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
    "    validation_data.loc[:, model_expected_features])\n",
    "live_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
    "    live_data.loc[:, model_expected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory of unused data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutralize our predictions to the riskiest features\n",
    "validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"preds_{model_name}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "\n",
    "live_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=live_data,\n",
    "    columns=[f\"preds_{model_name}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\n",
    "\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "live_data[\"prediction\"] = live_data[model_to_submit].rank(pct=True)\n",
    "validation_data[\"prediction\"].to_csv(f\"predictions/validation_predictions_{current_round}.csv\")\n",
    "live_data[\"prediction\"].to_csv(f\"predictions/live_predictions_{current_round}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "Done! Next steps:\n",
    "    1. Go to numer.ai/tournament (make sure you have an account)\n",
    "    2. Submit validation_predictions_{current_round}.csv to the diagnostics tool\n",
    "    3. Submit live_predictions_{current_round}.csv to the \"Upload Predictions\" button\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "numerai_base_model_v1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "projenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
