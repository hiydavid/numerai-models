{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUD4ylfaLZA6"
   },
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dioJyoTnxEm3"
   },
   "source": [
    "---\n",
    "# Load data & libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "MaNpcfNkKUg_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "from halo import Halo\n",
    "import gc\n",
    "import json\n",
    "from numerapi import NumerAPI\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    get_biggest_change_features,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# functions\n",
    "def get_biggest_change_features(corrs, n):\n",
    "    all_eras = corrs.index.sort_values()\n",
    "    h1_eras = all_eras[:len(all_eras) // 2]\n",
    "    h2_eras = all_eras[len(all_eras) // 2:]\n",
    "    h1_corr_means = corrs.loc[h1_eras, :].mean()\n",
    "    h2_corr_means = corrs.loc[h2_eras, :].mean()\n",
    "    corr_diffs = h2_corr_means - h1_corr_means\n",
    "    worst_n = corr_diffs.abs().sort_values(ascending=False).head(n).index.tolist()\n",
    "    return worst_n\n",
    "\n",
    "def load_model(name):\n",
    "    path = Path(f\"{MODEL_FOLDER}/{name}.pkl\")\n",
    "    if path.is_file():\n",
    "        model = pd.read_pickle(f\"{MODEL_FOLDER}/{name}.pkl\")\n",
    "    else:\n",
    "        model = False\n",
    "    return model\n",
    "\n",
    "def save_model(model, name):\n",
    "    try:\n",
    "        Path(MODEL_FOLDER).mkdir(exist_ok=True, parents=True)\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "    pd.to_pickle(model, f\"{MODEL_FOLDER}/{name}.pkl\")\n",
    "\n",
    "def neutralize(df, columns, neutralizers=None, proportion=1.0, normalize=True, era_col=\"era\"):\n",
    "    if neutralizers is None:\n",
    "        neutralizers = []\n",
    "    unique_eras = df[era_col].unique()\n",
    "    computed = []\n",
    "    for u in unique_eras:\n",
    "        df_era = df[df[era_col] == u]\n",
    "        scores = df_era[columns].values\n",
    "        if normalize:\n",
    "            scores2 = []\n",
    "            for x in scores.T:\n",
    "                x = (scipy.stats.rankdata(x, method='ordinal') - .5) / len(x)\n",
    "                x = scipy.stats.norm.ppf(x)\n",
    "                scores2.append(x)\n",
    "            scores = np.array(scores2).T\n",
    "        exposures = df_era[neutralizers].values\n",
    "        scores -= proportion * exposures.dot(\n",
    "            np.linalg.pinv(exposures.astype(np.float32)).dot(scores.astype(np.float32)))\n",
    "        scores /= scores.std(ddof=0)\n",
    "        computed.append(scores)\n",
    "    return pd.DataFrame(np.concatenate(computed), columns=columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current round #: 310\n"
     ]
    }
   ],
   "source": [
    "# instantiate numerai api\n",
    "napi = NumerAPI()\n",
    "current_round = napi.get_current_round()\n",
    "print(f\"Current round #: {current_round}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal training data\n"
     ]
    }
   ],
   "source": [
    "# read the feature metadata and get the \"small\" feature set\n",
    "print('Reading minimal training data')\n",
    "\n",
    "with open(\"data/features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "features = feature_metadata[\"feature_sets\"][\"small\"]\n",
    "read_columns = features + [ERA_COL, DATA_TYPE_COL, TARGET_COL]\n",
    "\n",
    "training_data = pd.read_parquet('data/training_data.parquet', columns=read_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the per era correlation of each feature vs the target\n",
    "all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
    "    lambda era: era[features].corrwith(era[TARGET_COL])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_undivorced_unsatisfying_praetorium',\n",
       " 'feature_exorbitant_myeloid_crinkle',\n",
       " 'feature_unvaried_social_bangkok',\n",
       " 'feature_haziest_lifelike_horseback',\n",
       " 'feature_lofty_acceptable_challenge',\n",
       " 'feature_unaired_operose_lactoprotein',\n",
       " 'feature_canalicular_peeling_lilienthal',\n",
       " 'feature_apomictical_motorized_vaporisation',\n",
       " 'feature_travelled_semipermeable_perruquier',\n",
       " 'feature_silver_handworked_scauper',\n",
       " 'feature_antichristian_slangiest_idyllist',\n",
       " 'feature_slack_calefacient_tableau',\n",
       " 'feature_jerkwater_eustatic_electrocardiograph',\n",
       " 'feature_unforbidden_highbrow_kafir',\n",
       " 'feature_flintier_enslaved_borsch',\n",
       " 'feature_assenting_darn_arthropod',\n",
       " 'feature_univalve_abdicant_distrail',\n",
       " 'feature_beery_somatologic_elimination',\n",
       " 'feature_bhutan_imagism_dolerite',\n",
       " 'feature_glare_factional_assessment',\n",
       " 'feature_unsealed_suffixal_babar',\n",
       " 'feature_grandmotherly_circumnavigable_homonymity',\n",
       " 'feature_winsome_irreproachable_milkfish',\n",
       " 'feature_branched_dilatory_sunbelt',\n",
       " 'feature_store_apteral_isocheim',\n",
       " 'feature_unwonted_trusted_fixative',\n",
       " 'feature_agile_unrespited_gaucho',\n",
       " 'feature_chuffier_analectic_conchiolin',\n",
       " 'feature_communicatory_unrecommended_velure',\n",
       " 'feature_planned_superimposed_bend',\n",
       " 'feature_crowning_frustrate_kampala',\n",
       " 'feature_stylistic_honduran_comprador',\n",
       " 'feature_buxom_curtained_sienna',\n",
       " 'feature_twisty_adequate_minutia',\n",
       " 'feature_moralistic_heartier_typhoid',\n",
       " 'feature_gullable_sanguine_incongruity',\n",
       " 'feature_introvert_symphysial_assegai',\n",
       " 'feature_cambial_bigoted_bacterioid']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n",
    "riskiest_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"garbage collection\" (gc) gets rid of unused data and frees up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'base_model'\n",
      "model not found, creating new one\n",
      "Training model: base_model\n",
      "saving new model: base_model\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"base_model\"\n",
    "print(f\"Checking for existing model '{model_name}'\")\n",
    "model = load_model(model_name)\n",
    "\n",
    "if not model:\n",
    "    print(f\"model not found, creating new one\")\n",
    "    params = {\"max_iter\": 2000,\n",
    "              \"learning_rate\": 0.01,\n",
    "              \"max_depth\": 5,\n",
    "              \"max_leaf_nodes\": 2 ** 5}\n",
    "\n",
    "    model = HistGradientBoostingRegressor(**params)\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    model.fit(\n",
    "        training_data.filter(like='feature_', axis='columns'),\n",
    "        training_data[TARGET_COL]\n",
    "    )\n",
    "    print(f\"saving new model: {model_name}\")\n",
    "    save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# clear memory of unused data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal features of validation and tournament data...\n"
     ]
    }
   ],
   "source": [
    "print('Reading minimal features of validation and tournament data...')\n",
    "validation_data = pd.read_parquet('data/validation_data.parquet', columns=read_columns)\n",
    "tournament_data = pd.read_parquet(f'data/tournament_data_{current_round}.parquet', columns=read_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nans per column this week: target_nomi_20    5351\n",
      "dtype: int64\n",
      "out of 5351 total rows\n",
      "filling nans with 0.5\n"
     ]
    }
   ],
   "source": [
    "nans_per_col = tournament_data[tournament_data[\"data_type\"] == \"live\"].isna().sum()\n",
    "\n",
    "if nans_per_col.any():\n",
    "    total_rows = len(tournament_data[tournament_data[\"data_type\"] == \"live\"])\n",
    "    print(f\"Number of nans per column this week: {nans_per_col[nans_per_col > 0]}\")\n",
    "    print(f\"out of {total_rows} total rows\")\n",
    "    print(f\"filling nans with 0.5\")\n",
    "    tournament_data.loc[:, features] = tournament_data.loc[:, features].fillna(0.5)\n",
    "else:\n",
    "    print(\"No nans in the features this week!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# check feature name\n",
    "model_expected_features = model.feature_names_in_\n",
    "\n",
    "if set(model_expected_features) != set(features):\n",
    "    print(f\"New features are available! Might want to retrain model {model_name}.\")\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
    "    validation_data.loc[:, model_expected_features])\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
    "    tournament_data.loc[:, model_expected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4361"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# clear memory of unused data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# neutralize our predictions to the riskiest features\n",
    "validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"preds_{model_name}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "\n",
    "tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=tournament_data,\n",
    "    columns=[f\"preds_{model_name}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\n",
    "validation_data[\"prediction\"].to_csv(f\"predictions/validation_predictions_{current_round}.csv\")\n",
    "tournament_data[\"prediction\"].to_csv(f\"predictions/tournament_predictions_{current_round}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Next steps:\n",
      "    1. Go to numer.ai/tournament (make sure you have an account)\n",
      "    2. Submit validation_predictions_310.csv to the diagnostics tool\n",
      "    3. Submit tournament_predictions_310.csv to the \"Upload Predictions\" button\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Done! Next steps:\n",
    "    1. Go to numer.ai/tournament (make sure you have an account)\n",
    "    2. Submit validation_predictions_{current_round}.csv to the diagnostics tool\n",
    "    3. Submit tournament_predictions_{current_round}.csv to the \"Upload Predictions\" button\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "numerai_base_model_v1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "projenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
